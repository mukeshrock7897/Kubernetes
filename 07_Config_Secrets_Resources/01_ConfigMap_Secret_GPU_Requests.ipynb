{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1942b781",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# âš™ï¸ **ConfigMap Â· Secret Â· GPU/CPU/RAM**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—‚ï¸ Config (non-secret) â†’ ConfigMap\n",
    "\n",
    "* Store plain configs (env vars, timeouts, limits).\n",
    "\n",
    "```yaml\n",
    "data:\n",
    "  MAX_TOKENS: \"512\"\n",
    "  GATEWAY_TIMEOUT: \"60\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ Secrets (tokens/keys) â†’ Secret\n",
    "\n",
    "* Store sensitive values (API keys, HF tokens).\n",
    "\n",
    "```yaml\n",
    "stringData:\n",
    "  HF_TOKEN: \"xxx\"\n",
    "  API_KEY: \"yyy\"\n",
    "```\n",
    "\n",
    "ğŸ‘‰ Mount as **env** or **files**.\n",
    "ğŸ‘‰ For private images â†’ use `imagePullSecrets`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¦ Use in Deployment\n",
    "\n",
    "```yaml\n",
    "envFrom:\n",
    "  - configMapRef: { name: app-cfg }\n",
    "  - secretRef:    { name: app-secret }\n",
    "volumeMounts:\n",
    "  - { name: secret-files, mountPath: /var/run/secret, readOnly: true }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“Š Resources (requests/limits)\n",
    "\n",
    "* Define **baseline + max usage** per pod.\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  requests: { cpu: \"2\", memory: \"16Gi\", nvidia.com/gpu: 1 }\n",
    "  limits:   { cpu: \"4\", memory: \"20Gi\", nvidia.com/gpu: 1 }\n",
    "```\n",
    "\n",
    "ğŸ‘‰ GPU requires **requests == limits**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ® GPU node scheduling\n",
    "\n",
    "```yaml\n",
    "tolerations:\n",
    "  - key: \"nvidia.com/gpu\" \n",
    "    operator: \"Exists\"\n",
    "    effect: \"NoSchedule\"\n",
    "nodeSelector:\n",
    "  accelerator: nvidia\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›¡ï¸ Security basics\n",
    "\n",
    "```yaml\n",
    "securityContext:\n",
    "  runAsNonRoot: true\n",
    "  allowPrivilegeEscalation: false\n",
    "  capabilities: { drop: [\"ALL\"] }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Sizing hints (LLM pods)\n",
    "\n",
    "* **CPU gateway** â†’ `0.5â€“2 CPU`, `512Miâ€“2Gi RAM`\n",
    "* **GPU inference** â†’ `2â€“4 CPU`, `16â€“32Gi RAM`, `1 GPU`\n",
    "* Tune throughput via â†’ vLLM `--max-num-seqs`, TGI `--max-batch-size`.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ” Quick ops\n",
    "\n",
    "```bash\n",
    "kubectl -n llm get cm,secret\n",
    "kubectl -n llm describe deploy vllm\n",
    "kubectl -n llm get pods -o=custom-columns=NAME:.metadata.name,CPU:.spec.containers[*].resources.requests.cpu,MEM:.spec.containers[*].resources.requests.memory\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "âœ… Rule: **Configs â†’ ConfigMap**, **Secrets â†’ Secret**, **GPU via nvidia.com/gpu**, always add **securityContext**.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
