{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1942b781",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ⚙️ **ConfigMap · Secret · GPU/CPU/RAM**\n",
    "\n",
    "---\n",
    "\n",
    "## 🗂️ Config (non-secret) → ConfigMap\n",
    "\n",
    "* Store plain configs (env vars, timeouts, limits).\n",
    "\n",
    "```yaml\n",
    "data:\n",
    "  MAX_TOKENS: \"512\"\n",
    "  GATEWAY_TIMEOUT: \"60\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 Secrets (tokens/keys) → Secret\n",
    "\n",
    "* Store sensitive values (API keys, HF tokens).\n",
    "\n",
    "```yaml\n",
    "stringData:\n",
    "  HF_TOKEN: \"xxx\"\n",
    "  API_KEY: \"yyy\"\n",
    "```\n",
    "\n",
    "👉 Mount as **env** or **files**.\n",
    "👉 For private images → use `imagePullSecrets`.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 Use in Deployment\n",
    "\n",
    "```yaml\n",
    "envFrom:\n",
    "  - configMapRef: { name: app-cfg }\n",
    "  - secretRef:    { name: app-secret }\n",
    "volumeMounts:\n",
    "  - { name: secret-files, mountPath: /var/run/secret, readOnly: true }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 Resources (requests/limits)\n",
    "\n",
    "* Define **baseline + max usage** per pod.\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  requests: { cpu: \"2\", memory: \"16Gi\", nvidia.com/gpu: 1 }\n",
    "  limits:   { cpu: \"4\", memory: \"20Gi\", nvidia.com/gpu: 1 }\n",
    "```\n",
    "\n",
    "👉 GPU requires **requests == limits**.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎮 GPU node scheduling\n",
    "\n",
    "```yaml\n",
    "tolerations:\n",
    "  - key: \"nvidia.com/gpu\" \n",
    "    operator: \"Exists\"\n",
    "    effect: \"NoSchedule\"\n",
    "nodeSelector:\n",
    "  accelerator: nvidia\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🛡️ Security basics\n",
    "\n",
    "```yaml\n",
    "securityContext:\n",
    "  runAsNonRoot: true\n",
    "  allowPrivilegeEscalation: false\n",
    "  capabilities: { drop: [\"ALL\"] }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 📐 Sizing hints (LLM pods)\n",
    "\n",
    "* **CPU gateway** → `0.5–2 CPU`, `512Mi–2Gi RAM`\n",
    "* **GPU inference** → `2–4 CPU`, `16–32Gi RAM`, `1 GPU`\n",
    "* Tune throughput via → vLLM `--max-num-seqs`, TGI `--max-batch-size`.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Quick ops\n",
    "\n",
    "```bash\n",
    "kubectl -n llm get cm,secret\n",
    "kubectl -n llm describe deploy vllm\n",
    "kubectl -n llm get pods -o=custom-columns=NAME:.metadata.name,CPU:.spec.containers[*].resources.requests.cpu,MEM:.spec.containers[*].resources.requests.memory\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "✅ Rule: **Configs → ConfigMap**, **Secrets → Secret**, **GPU via nvidia.com/gpu**, always add **securityContext**.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
