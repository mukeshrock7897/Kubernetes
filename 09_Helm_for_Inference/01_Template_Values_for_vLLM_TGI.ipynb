{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61ceedb4",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ™ **Helm Template & Values (vLLM / TGI)**\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ What this chart does\n",
    "\n",
    "* ğŸš€ Deploys **LLM inference servers** (vLLM or TGI)\n",
    "* âš¡ Includes GPU requests, PVC for models, health probes\n",
    "* ğŸŒ Exposes via **Service**, optional Ingress\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‚ Chart layout\n",
    "\n",
    "```\n",
    "llm-inference/\n",
    "  Chart.yaml\n",
    "  values.yaml\n",
    "  templates/\n",
    "    deployment.yaml\n",
    "    service.yaml\n",
    "    pvc.yaml   # only if persistence.create=true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš™ï¸ Values (examples)\n",
    "\n",
    "**vLLM**\n",
    "\n",
    "```yaml\n",
    "image: vllm/vllm-openai:latest\n",
    "args: [\"--model\",\"/models/Llama-3-8B-Instruct\",\"--max-num-seqs\",\"8\"]\n",
    "resources: { requests: { cpu: 2, memory: 16Gi, nvidia.com/gpu: 1 } }\n",
    "persistence: { create: false, existingClaim: models-rwx }\n",
    "```\n",
    "\n",
    "**TGI**\n",
    "\n",
    "```yaml\n",
    "image: ghcr.io/huggingface/text-generation-inference:latest\n",
    "args: [\"--model-id\",\"/models/Llama-3-8B-Instruct\",\"--max-batch-size\",\"8\"]\n",
    "resources: { requests: { cpu: 2, memory: 16Gi, nvidia.com/gpu: 1 } }\n",
    "persistence: { create: false, existingClaim: models-rwx }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“ Templates (minimal)\n",
    "\n",
    "**Deployment**\n",
    "\n",
    "```yaml\n",
    "containers:\n",
    "  - name: api\n",
    "    image: {{ .Values.image }}\n",
    "    args: {{- toYaml .Values.args | nindent 10 }}\n",
    "    ports: [{ containerPort: {{ .Values.port }} }]\n",
    "    readinessProbe: { httpGet: { path: /health, port: {{ .Values.port }} } }\n",
    "    volumeMounts:\n",
    "      - { name: model, mountPath: /models, readOnly: true }\n",
    "volumes:\n",
    "  - name: model\n",
    "    persistentVolumeClaim:\n",
    "      claimName: {{ .Values.persistence.existingClaim }}\n",
    "```\n",
    "\n",
    "**Service**\n",
    "\n",
    "```yaml\n",
    "ports: [{ port: {{ .Values.service.port }}, targetPort: {{ .Values.port }} }]\n",
    "type: {{ .Values.service.type }}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ Install / Upgrade\n",
    "\n",
    "```bash\n",
    "helm install vllm ./llm-inference -n llm -f values.vllm.yaml\n",
    "helm install tgi  ./llm-inference -n llm -f values.tgi.yaml\n",
    "helm upgrade vllm ./llm-inference -n llm -f values.vllm.yaml\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§  LLM Notes\n",
    "\n",
    "* ğŸ“‚ Mount `/models:ro` from PVC (RWX preferred).\n",
    "* ğŸ› Tune throughput flags first (vLLM `--max-num-seqs`, TGI `--max-batch-size`).\n",
    "* ğŸ”’ Use immutable image tags (avoid `:latest`).\n",
    "* ğŸ“ˆ Add HPA/KEDA later for autoscaling.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
