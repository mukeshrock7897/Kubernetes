{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21a70d6e",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üìà **Deployments ¬∑ HPA ¬∑ KEDA**\n",
    "\n",
    "---\n",
    "\n",
    "## üåÄ Deployment (baseline)\n",
    "\n",
    "* Keeps **N replicas alive** + rolls updates safely.\n",
    "* Scale manually:\n",
    "\n",
    "```bash\n",
    "kubectl scale deploy/vllm --replicas=3 -n llm\n",
    "```\n",
    "\n",
    "**Throughput knobs (tune inside pod first):**\n",
    "\n",
    "* ‚ö° vLLM ‚Üí `--max-num-seqs`, `--gpu-memory-utilization`\n",
    "* ‚ö° TGI ‚Üí `--max-batch-size`, `--max-concurrent-requests`\n",
    "\n",
    "---\n",
    "\n",
    "## üìä HPA (Horizontal Pod Autoscaler)\n",
    "\n",
    "* Auto-scales on **CPU/RAM** usage.\n",
    "* Good **baseline autoscaling** when no custom metrics.\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata: { name: vllm-hpa, namespace: llm }\n",
    "spec:\n",
    "  scaleTargetRef: { apiVersion: apps/v1, kind: Deployment, name: vllm }\n",
    "  minReplicas: 1\n",
    "  maxReplicas: 5\n",
    "  metrics:\n",
    "    - type: Resource\n",
    "      resource:\n",
    "        name: cpu\n",
    "        target: { type: Utilization, averageUtilization: 70 }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ KEDA (advanced autoscaling)\n",
    "\n",
    "* Scale on **real LLM signals** ‚Üí requests/sec, queue depth, tokens/sec.\n",
    "* Works with **Prometheus**, **Redis**, **Kafka**, etc.\n",
    "\n",
    "üîπ **Prometheus (requests in flight):**\n",
    "\n",
    "```yaml\n",
    "triggers:\n",
    "  - type: prometheus\n",
    "    metadata:\n",
    "      query: sum(rate(http_requests_in_flight{app=\"vllm\"}[1m]))\n",
    "      threshold: \"20\"\n",
    "```\n",
    "\n",
    "üîπ **Redis (embedding workers):**\n",
    "\n",
    "```yaml\n",
    "triggers:\n",
    "  - type: redis\n",
    "    metadata:\n",
    "      listName: embed-jobs\n",
    "      listLength: \"100\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ°Ô∏è Availability guardrails\n",
    "\n",
    "* **PDB (PodDisruptionBudget)** ‚Üí keep ‚â•1 pod during maintenance.\n",
    "* **Tip:** set `maxUnavailable: 0` for zero-downtime rollouts.\n",
    "* Always use **readinessProbe** ‚Üí avoid routing to cold pods.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö° Quick ops\n",
    "\n",
    "```bash\n",
    "kubectl top pods -n llm            # needs metrics-server\n",
    "kubectl rollout status deploy/vllm # watch rollout\n",
    "kubectl describe hpa vllm-hpa -n llm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Rule of thumb\n",
    "\n",
    "1. Tune **batch/concurrency flags**.\n",
    "2. Start with fixed **replicas**.\n",
    "3. Add **HPA** (CPU/RAM).\n",
    "4. Upgrade to **KEDA** (real workload metrics).\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
