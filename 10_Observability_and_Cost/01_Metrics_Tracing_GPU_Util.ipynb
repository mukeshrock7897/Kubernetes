{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e074c3bd",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# üìä **Metrics ¬∑ Tracing ¬∑ GPU Util**\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What to measure (LLM apps)\n",
    "\n",
    "* ‚ö° **Throughput** ‚Üí tokens/sec, requests/sec, queue depth\n",
    "* ‚è± **Latency** ‚Üí p50/p95\n",
    "* ‚ùå **Errors** ‚Üí 4xx/5xx, timeouts\n",
    "* üéÆ **GPU** ‚Üí util %, VRAM used\n",
    "* üìÇ **Cache/IO** ‚Üí HF cache hit %, PV read\n",
    "\n",
    "---\n",
    "\n",
    "## üõ† Quick setup\n",
    "\n",
    "* üü¢ **metrics-server** ‚Üí `kubectl top pods`\n",
    "* üìà **Prometheus + Grafana** ‚Üí dashboards\n",
    "* üéÆ **DCGM exporter** ‚Üí GPU metrics\n",
    "\n",
    "üëâ Start Prometheus+Grafana ‚Üí add DCGM for GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© App metrics (FastAPI example)\n",
    "\n",
    "Expose `/metrics` with Prometheus counters:\n",
    "\n",
    "```python\n",
    "TOKENS = Counter(\"tokens_total\",\"tokens emitted\")\n",
    "LAT    = Histogram(\"latency_seconds\",\"request latency\")\n",
    "```\n",
    "\n",
    "üëâ Mount via `/metrics` for scraping.\n",
    "\n",
    "---\n",
    "\n",
    "## üì° Prometheus scrape (sketch)\n",
    "\n",
    "```yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "spec:\n",
    "  selector: { matchLabels: { app: gateway } }\n",
    "  endpoints:\n",
    "    - port: metrics\n",
    "      interval: 15s\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üìä Grafana queries\n",
    "\n",
    "* Tokens/sec ‚Üí `sum(rate(tokens_total[1m]))`\n",
    "* Error rate ‚Üí `5xx / total`\n",
    "* p95 latency ‚Üí `histogram_quantile(0.95, ‚Ä¶ )`\n",
    "* GPU util ‚Üí `avg(DCGM_FI_DEV_GPU_UTIL)`\n",
    "* VRAM used ‚Üí `avg(DCGM_FI_DEV_FB_USED)`\n",
    "\n",
    "---\n",
    "\n",
    "## üîé Tracing (optional)\n",
    "\n",
    "* Add **OpenTelemetry** ‚Üí gateway spans only.\n",
    "* Export to **Tempo/Jaeger** for debugging long-tail latency.\n",
    "\n",
    "---\n",
    "\n",
    "## üìà Scaling signals (KEDA)\n",
    "\n",
    "Use **requests in flight**, **queue depth**, or **tokens/sec per replica**.\n",
    "\n",
    "```yaml\n",
    "- type: prometheus\n",
    "  metadata:\n",
    "    metricName: requests_in_flight\n",
    "    query: sum(app_inflight)\n",
    "    threshold: \"20\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üí∞ Cost formula\n",
    "\n",
    "```\n",
    "cost_per_1k_tokens ‚âà gpu_hourly_cost / (tokens_per_sec * 3.6)\n",
    "```\n",
    "\n",
    "üëâ Goal: high GPU util + good p95 latency.\n",
    "\n",
    "---\n",
    "\n",
    "## üö® Alerts (tiny start)\n",
    "\n",
    "* Error rate > 2%\n",
    "* p95 latency > SLO\n",
    "* GPU util <30% (wasted) or >98% (hot)\n",
    "* VRAM >90% (OOM risk)\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Quick ops\n",
    "\n",
    "```bash\n",
    "kubectl -n monitoring get pods       # prom/grafana\n",
    "kubectl -n llm logs deploy/gateway   # gateway logs\n",
    "kubectl -n llm port-forward svc/grafana 3000:80\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
