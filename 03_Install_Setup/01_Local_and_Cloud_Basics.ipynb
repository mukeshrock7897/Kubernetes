{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c4ce5dc",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 🖥️ **Local & Cloud Basics (GPU prereqs)**\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠️ Tools (install once)\n",
    "\n",
    "* **kubectl** → manage clusters\n",
    "* **Docker** → build & run images\n",
    "* **Helm** *(optional)* → package deployments\n",
    "* ⚡ Enable faster builds → `export DOCKER_BUILDKIT=1`\n",
    "\n",
    "---\n",
    "\n",
    "## 💻 Local CPU (quickest dev)\n",
    "\n",
    "```bash\n",
    "kind create cluster\n",
    "kubectl create ns llm\n",
    "kubectl -n llm create deploy echo --image=ealen/echo-server --port=80\n",
    "kubectl -n llm expose deploy echo --port=80\n",
    "kubectl -n llm port-forward svc/echo 8080:80\n",
    "```\n",
    "\n",
    "👉 Test at `http://localhost:8080`\n",
    "\n",
    "🍏 Mac (Apple Silicon): build for amd64\n",
    "\n",
    "```bash\n",
    "docker buildx build --platform linux/amd64 -t user/app:dev .\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ☁️ Cloud GPU (checklist)\n",
    "\n",
    "1. Provision cluster + **GPU node pool** (EKS/GKE/AKS).\n",
    "2. Install **NVIDIA device plugin**.\n",
    "3. Mount model storage → **RWX PVC** (best) or RWO + initContainer.\n",
    "4. Label/taint GPU nodes → schedule pods correctly.\n",
    "\n",
    "**GPU pod snippet**\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  requests: { nvidia.com/gpu: 1, cpu: \"2\", memory: \"16Gi\" }\n",
    "  limits:   { nvidia.com/gpu: 1, cpu: \"4\", memory: \"20Gi\" }\n",
    "```\n",
    "\n",
    "**Quick GPU sanity check**\n",
    "\n",
    "```bash\n",
    "kubectl run -it --rm gpu-test \\\n",
    "  --image=nvidia/cuda:12.3.2-base-ubuntu22.04 \\\n",
    "  --limits=nvidia.com/gpu=1 -- nvidia-smi\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💾 Storage for Models\n",
    "\n",
    "* **📂 RWX PVC** → shared, mount `/models:ro` (best).\n",
    "* **📥 RWO + initContainer** → download per pod at startup.\n",
    "* **⚡ HuggingFace cache** → keep on PVC to avoid re-downloads.\n",
    "\n",
    "**PVC Example**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata: { name: models-pvc, namespace: llm }\n",
    "spec:\n",
    "  accessModes: [ReadWriteMany]\n",
    "  resources: { requests: { storage: 200Gi } }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 Basic Ops (verify cluster)\n",
    "\n",
    "```bash\n",
    "kubectl get nodes -o wide          # check nodes + GPU labels\n",
    "kubectl get pods -A                # see all pods\n",
    "kubectl -n llm logs deploy/echo    # check logs\n",
    "kubectl -n llm describe deploy vllm\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
