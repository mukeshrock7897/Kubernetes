{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b700a1d8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# ğŸ§  Why Kubernetes for LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ When to use Kubernetes (LLMs)\n",
    "\n",
    "* **âš¡ Scale & uptime** â†’ auto-healing, zero-downtime rollouts.\n",
    "* **ğŸ® GPU scheduling** â†’ pool GPUs, place pods on GPU nodes.\n",
    "* **ğŸ”— Multi-service graphs** â†’ gateway â†” LLM â†” embeddings â†” vector DB â†” cache.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ¯ When K8s helps (LLM scenarios)\n",
    "\n",
    "* **ğŸ“ˆ Bursting traffic** â†’ replicas scale up/down with load.\n",
    "* **ğŸ–¥ï¸ GPU pooling** â†’ taints/tolerations keep GPU pods on GPU nodes.\n",
    "* **ğŸ”€ Multi-service pipelines** â†’ chain gateway â†” LLM â†” rewriter â†” embeddings â†” vector DB.\n",
    "* **ğŸ›¡ï¸ Ops guardrails** â†’ probes, limits, rollbacks, disruption budgets.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ—ï¸ Minimal Reference Architecture\n",
    "\n",
    "```\n",
    "Client â†’ Ingress â†’ Gateway(API) â†’ LLM Inference (GPU)\n",
    "                         â”‚\n",
    "                         â”œâ”€ Embeddings Service (CPU)\n",
    "                         â”œâ”€ Vector DB (Milvus/Qdrant/pgvector)\n",
    "                         â””â”€ Cache/Queue (Redis/Kafka)\n",
    "```\n",
    "\n",
    "* **ğŸ“‚ Artifacts** â†’ models on PVC (RWX preferred) or pulled by initContainers.\n",
    "* **ğŸ–§ Node pools** â†’ GPU (tainted) + CPU (general).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”‘ K8s pieces (LLM mapping)\n",
    "\n",
    "* **ğŸ“¦ Pod** â†’ one inference instance.\n",
    "* **ğŸŒ€ Deployment** â†’ manages replicas + rolling updates.\n",
    "* **ğŸŒ Service** â†’ stable in-cluster endpoint.\n",
    "* **ğŸšª Ingress/LB** â†’ public entry to the cluster.\n",
    "* **ğŸ’¾ PVC** â†’ persistent model weights/cache.\n",
    "* **ğŸ“Š HPA/KEDA** â†’ autoscale on CPU/RAM or RPS/tokens/sec.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“‚ Model storage options\n",
    "\n",
    "* **ğŸ—„ï¸ RWX PVC** â†’ shared, read-only, best for models.\n",
    "* **ğŸ“¥ InitContainer + RWO PVC** â†’ download model at pod startup.\n",
    "* **ğŸ“¦ Warm image** â†’ bake weights in image (fast start, heavy build).\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ“¡ Ops signals to watch\n",
    "\n",
    "* **âš¡ Throughput** â†’ tokens/sec.\n",
    "* **â±ï¸ Latency** â†’ p95 response time.\n",
    "* **ğŸ® GPU util** â†’ % + VRAM usage.\n",
    "* **ğŸ“Œ Queue depth** â†’ requests in-flight.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ›¡ï¸ Safety basics\n",
    "\n",
    "* **ğŸ©º Probes** â†’ readiness, liveness, startup.\n",
    "* **ğŸ“ Requests/Limits** â†’ CPU, RAM, GPU guardrails.\n",
    "* **ğŸ”’ Immutable images** â†’ avoid `:latest`, pin tags/digests.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ§ª 60-sec local smoke (CPU)\n",
    "\n",
    "```bash\n",
    "kind create cluster\n",
    "kubectl create ns llm\n",
    "kubectl -n llm create deploy echo --image=ealen/echo-server --port=80\n",
    "kubectl -n llm expose deploy echo --port=80\n",
    "kubectl -n llm port-forward svc/echo 8080:80\n",
    "# test: curl http://localhost:8080\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
