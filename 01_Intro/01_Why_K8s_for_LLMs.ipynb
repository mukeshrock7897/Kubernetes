{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b700a1d8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 🧠 Why Kubernetes for LLMs\n",
    "\n",
    "---\n",
    "\n",
    "## 🚀 When to use Kubernetes (LLMs)\n",
    "\n",
    "* **⚡ Scale & uptime** → auto-healing, zero-downtime rollouts.\n",
    "* **🎮 GPU scheduling** → pool GPUs, place pods on GPU nodes.\n",
    "* **🔗 Multi-service graphs** → gateway ↔ LLM ↔ embeddings ↔ vector DB ↔ cache.\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 When K8s helps (LLM scenarios)\n",
    "\n",
    "* **📈 Bursting traffic** → replicas scale up/down with load.\n",
    "* **🖥️ GPU pooling** → taints/tolerations keep GPU pods on GPU nodes.\n",
    "* **🔀 Multi-service pipelines** → chain gateway ↔ LLM ↔ rewriter ↔ embeddings ↔ vector DB.\n",
    "* **🛡️ Ops guardrails** → probes, limits, rollbacks, disruption budgets.\n",
    "\n",
    "---\n",
    "\n",
    "## 🏗️ Minimal Reference Architecture\n",
    "\n",
    "```\n",
    "Client → Ingress → Gateway(API) → LLM Inference (GPU)\n",
    "                         │\n",
    "                         ├─ Embeddings Service (CPU)\n",
    "                         ├─ Vector DB (Milvus/Qdrant/pgvector)\n",
    "                         └─ Cache/Queue (Redis/Kafka)\n",
    "```\n",
    "\n",
    "* **📂 Artifacts** → models on PVC (RWX preferred) or pulled by initContainers.\n",
    "* **🖧 Node pools** → GPU (tainted) + CPU (general).\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 K8s pieces (LLM mapping)\n",
    "\n",
    "* **📦 Pod** → one inference instance.\n",
    "* **🌀 Deployment** → manages replicas + rolling updates.\n",
    "* **🌐 Service** → stable in-cluster endpoint.\n",
    "* **🚪 Ingress/LB** → public entry to the cluster.\n",
    "* **💾 PVC** → persistent model weights/cache.\n",
    "* **📊 HPA/KEDA** → autoscale on CPU/RAM or RPS/tokens/sec.\n",
    "\n",
    "---\n",
    "\n",
    "## 📂 Model storage options\n",
    "\n",
    "* **🗄️ RWX PVC** → shared, read-only, best for models.\n",
    "* **📥 InitContainer + RWO PVC** → download model at pod startup.\n",
    "* **📦 Warm image** → bake weights in image (fast start, heavy build).\n",
    "\n",
    "---\n",
    "\n",
    "## 📡 Ops signals to watch\n",
    "\n",
    "* **⚡ Throughput** → tokens/sec.\n",
    "* **⏱️ Latency** → p95 response time.\n",
    "* **🎮 GPU util** → % + VRAM usage.\n",
    "* **📌 Queue depth** → requests in-flight.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛡️ Safety basics\n",
    "\n",
    "* **🩺 Probes** → readiness, liveness, startup.\n",
    "* **📐 Requests/Limits** → CPU, RAM, GPU guardrails.\n",
    "* **🔒 Immutable images** → avoid `:latest`, pin tags/digests.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 60-sec local smoke (CPU)\n",
    "\n",
    "```bash\n",
    "kind create cluster\n",
    "kubectl create ns llm\n",
    "kubectl -n llm create deploy echo --image=ealen/echo-server --port=80\n",
    "kubectl -n llm expose deploy echo --port=80\n",
    "kubectl -n llm port-forward svc/echo 8080:80\n",
    "# test: curl http://localhost:8080\n",
    "```\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
