{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3059c584",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# 📦 **Pods · Deployments · Services (LLM lens)**\n",
    "\n",
    "---\n",
    "\n",
    "## 🔑 TL;DR\n",
    "\n",
    "* **🧩 Pod** → one inference instance (vLLM/TGI).\n",
    "* **🌀 Deployment** → manages replicas & rolling updates.\n",
    "* **🌐 Service** → stable endpoint; add Ingress/LB for public access.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧩 Pod\n",
    "\n",
    "* Runs 1+ containers with shared IP/volumes.\n",
    "* **Ephemeral** → don’t store data inside; mount **PVC** for models/cache.\n",
    "\n",
    "---\n",
    "\n",
    "## 🌀 Deployment\n",
    "\n",
    "* Ensures **N replicas** and safe updates.\n",
    "* Scale with:\n",
    "\n",
    "```bash\n",
    "kubectl scale deploy/vllm --replicas=3 -n llm\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 🌐 Service\n",
    "\n",
    "* **ClusterIP** → internal only.\n",
    "* **LoadBalancer** → cloud public IP.\n",
    "* **Ingress** → HTTP routing + TLS.\n",
    "\n",
    "---\n",
    "\n",
    "## 🛠 Minimal Example\n",
    "\n",
    "**Deployment**\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata: { name: vllm, namespace: llm }\n",
    "spec:\n",
    "  replicas: 1\n",
    "  selector: { matchLabels: { app: vllm } }\n",
    "  template:\n",
    "    metadata: { labels: { app: vllm } }\n",
    "    spec:\n",
    "      containers:\n",
    "        - name: api\n",
    "          image: vllm/vllm-openai:latest\n",
    "          args: [\"--model\",\"/models/Llama-3-8B-Instruct\",\"--max-num-seqs\",\"8\"]\n",
    "          ports: [{ containerPort: 8000 }]\n",
    "          volumeMounts:\n",
    "            - { name: model, mountPath: /models, readOnly: true }\n",
    "      volumes:\n",
    "        - name: model\n",
    "          persistentVolumeClaim: { claimName: models-pvc }\n",
    "```\n",
    "\n",
    "**Service**\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata: { name: vllm-svc, namespace: llm }\n",
    "spec:\n",
    "  selector: { app: vllm }\n",
    "  ports: [{ port: 80, targetPort: 8000 }]\n",
    "  type: ClusterIP\n",
    "```\n",
    "\n",
    "👉 Local test:\n",
    "\n",
    "```bash\n",
    "kubectl -n llm port-forward svc/vllm-svc 8080:80\n",
    "curl http://localhost:8080/health\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 💡 LLM Tips\n",
    "\n",
    "* Mount models via **PVC (read-only)**.\n",
    "* Use **immutable image tags** (no `:latest`).\n",
    "* Add **probes & resources** early; autoscale later with HPA/KEDA.\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
